{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.65.0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.4)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110683 sha256=49e95ae7cf9eef424a91a150c5862753f21a2d06082dcf4b98b3cf4037c5ca0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/4b/fb/736478af5e8004810081a06259f9aa2f7c3329fc5d03c2c412\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "\u001b[33m  WARNING: The script slugify is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script kaggle is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed kaggle-1.5.16 python-slugify-8.0.1 text-unidecode-1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --user kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['HOME'] + '/.local/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading tabular-playground-series-nov-2022.zip to /root\n",
      " 99%|██████████████████████████████████████▋| 1.05G/1.06G [00:11<00:00, 108MB/s]\n",
      "100%|██████████████████████████████████████| 1.06G/1.06G [00:12<00:00, 94.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!~/.local/bin/kaggle competitions download -c tabular-playground-series-nov-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 12 not upgraded.\n",
      "Need to get 168 kB of archives.\n",
      "After this operation, 593 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.1 [168 kB]\n",
      "Fetched 168 kB in 1s (329 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 17208 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1.1_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1.1) ...\n",
      "Setting up unzip (6.0-25ubuntu1.1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update -y && apt-get install -y unzip\n",
    "!unzip -d tabular-playground-series-nov-2022 -q tabular-playground-series-nov-2022.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-11T06:00:38.471482Z",
     "iopub.status.busy": "2023-12-11T06:00:38.471067Z",
     "iopub.status.idle": "2023-12-11T06:00:41.738323Z",
     "shell.execute_reply": "2023-12-11T06:00:41.736887Z",
     "shell.execute_reply.started": "2023-12-11T06:00:38.471450Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "id          \n",
       "0          0\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          0\n",
       "...      ...\n",
       "19995      1\n",
       "19996      1\n",
       "19997      0\n",
       "19998      0\n",
       "19999      1\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "train_labels = pd.read_csv('tabular-playground-series-nov-2022/train_labels.csv', index_col='id')\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "base_dir = 'tabular-playground-series-nov-2022/submission_files/'\n",
    "files = {}\n",
    "print(len(os.listdir(base_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pred\n",
      "id             \n",
      "0      0.709336\n",
      "1      0.452988\n",
      "2      0.675462\n",
      "3      0.481046\n",
      "4      0.957339\n",
      "...         ...\n",
      "39995  0.382515\n",
      "39996  0.352498\n",
      "39997  0.577554\n",
      "39998  0.712353\n",
      "39999  0.483215\n",
      "\n",
      "[40000 rows x 1 columns]\n",
      "id\n",
      "0        0.709336\n",
      "1        0.452988\n",
      "2        0.675462\n",
      "3        0.481046\n",
      "4        0.957339\n",
      "           ...   \n",
      "39995    0.382515\n",
      "39996    0.352498\n",
      "39997    0.577554\n",
      "39998    0.712353\n",
      "39999    0.483215\n",
      "Name: pred, Length: 40000, dtype: float64\n",
      "0.587543\n",
      "       label      pred\n",
      "id                    \n",
      "0          0  0.709336\n",
      "1          1  0.452988\n",
      "2          1  0.675462\n",
      "3          1  0.481046\n",
      "4          0  0.957339\n",
      "...      ...       ...\n",
      "19995      1  0.685909\n",
      "19996      1  0.645524\n",
      "19997      0  0.733537\n",
      "19998      0  0.487501\n",
      "19999      1  0.972671\n",
      "\n",
      "[20000 rows x 2 columns]\n",
      "[0 1 1 ... 0 0 1]\n",
      "[0.709336 0.452988 0.675462 ... 0.733537 0.487501 0.972671]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_981/3417020417.py:16: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = -np.mean(label * np.log2(pred) + (1-label) * np.log2(1-pred))\n",
      "/tmp/ipykernel_981/3417020417.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  result = -np.mean(label * np.log2(pred) + (1-label) * np.log2(1-pred))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for file in os.listdir(base_dir):\n",
    "    data = pd.read_csv(os.path.join(base_dir, file), index_col='id')\n",
    "    files[file] = data\n",
    "    print(data)\n",
    "    print(data.pred)\n",
    "    print(data.pred.loc[15])\n",
    "    merged = pd.merge(train_labels, data, left_index=True, right_index=True)\n",
    "    print(merged)\n",
    "    label = merged.label.to_numpy()\n",
    "    pred = merged.pred.to_numpy()\n",
    "    print(label)\n",
    "    print(pred)\n",
    "    # pred: probability that label is 1\n",
    "    # 1-pred: probability that label is 0\n",
    "    result = -np.mean(label * np.log2(pred) + (1-label) * np.log2(1-pred))\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "files = os.listdir(base_dir)\n",
    "dataarr = []\n",
    "for i, file in enumerate(files):\n",
    "    data = pd.read_csv(os.path.join(base_dir, file), index_col='id')\n",
    "    dataarr.append(data.pred.to_numpy())\n",
    "dataarr = np.array(dataarr)\n",
    "np.save(f\"dataarr.npy\", dataarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 40000)\n"
     ]
    }
   ],
   "source": [
    "print(dataarr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_arr = train_labels.label.to_numpy()\n",
    "train_labels_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_981/4294781021.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  loss = -np.mean(train_labels_arr * np.log2(cur_labels) + (1-train_labels_arr) * np.log2(1-cur_labels))\n",
      "/tmp/ipykernel_981/4294781021.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.mean(train_labels_arr * np.log2(cur_labels) + (1-train_labels_arr) * np.log2(1-cur_labels))\n",
      "/tmp/ipykernel_981/4294781021.py:4: RuntimeWarning: invalid value encountered in log2\n",
      "  loss = -np.mean(train_labels_arr * np.log2(cur_labels) + (1-train_labels_arr) * np.log2(1-cur_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4194\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(len(dataarr)):\n",
    "    cur_labels = dataarr[i, :20000]\n",
    "    loss = -np.mean(train_labels_arr * np.log2(cur_labels) + (1-train_labels_arr) * np.log2(1-cur_labels))\n",
    "    if not np.isnan(loss):\n",
    "        losses.append(loss)\n",
    "losses = np.array(losses)\n",
    "# print(sorted(losses))\n",
    "print(len(losses))\n",
    "#     print(cur_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 20000])\n",
      "torch.Size([5000])\n",
      "tensor([0.8766, 0.8767, 0.8791,  ..., 1.0571,    nan, 0.9786], device='cuda:0')\n",
      "tensor(4930, device='cuda:0')\n",
      "tensor(0.8766, device='cuda:0')\n",
      "tensor(4930, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda'\n",
    "train_input = torch.tensor(dataarr, device=device, dtype=torch.float)[:, :20000].contiguous()\n",
    "print(train_input.shape)\n",
    "train_labels_torch = torch.tensor(train_labels_arr, device=device, dtype=torch.long)\n",
    "losses = train_labels_torch.unsqueeze(0) * torch.log2(train_input*0.99+0.01) + (1-train_labels_torch.unsqueeze(0)) * torch.log2(1-train_input*0.99)\n",
    "losses = -losses.mean(dim=-1)\n",
    "print(losses.shape)\n",
    "print(losses)\n",
    "print((~losses.isnan()).sum())\n",
    "print(losses[~losses.isnan()].min())\n",
    "valid_inputs = losses <= 1.1\n",
    "print(valid_inputs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 4930])\n",
      "tensor([0.8922, 1.3801, 1.7495,  ..., 0.9936, 1.8821, 1.3069], device='cuda:0')\n",
      "tensor(2034, device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor(inf, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8922,  1.3801,  1.7495,  ...,  0.9936,  1.8821,  1.3069],\n",
       "        [-0.1886, -0.5561,  0.2720,  ...,  1.4958,  1.4777,  0.1886],\n",
       "        [ 0.7330,  1.6751,  1.3864,  ...,  1.4299,  2.1542,  0.6946],\n",
       "        ...,\n",
       "        [ 1.0126,  2.5936,  2.1835,  ...,  1.4689,  2.9751,  1.0669],\n",
       "        [-0.0500,  0.1375,  0.7751,  ...,  1.3543,  0.9965,  0.0520],\n",
       "        [ 3.5721,  3.2995,  5.0000,  ...,  2.4498,  4.3919,  2.0806]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid: y=1/(1+exp(-x))\n",
    "# do sigmoid inverse: x=log(y/(1-y))\n",
    "train_input_x = train_input[valid_inputs].permute(1, 0)\n",
    "train_input_x = torch.log(train_input_x / (1 - train_input_x))\n",
    "print(train_input_x.shape)\n",
    "print(train_input_x[0])\n",
    "print(train_input_x.isnan().sum())\n",
    "train_input_x[train_input_x.isnan()] = 0\n",
    "print(train_input_x.min(), train_input_x.max())\n",
    "train_input_x.clamp_(-5, 5)\n",
    "print(train_input_x.min(), train_input_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "9584\n",
      "0 nan\n",
      "epoch 0 nan\n",
      "epoch 1 nan\n",
      "epoch 2 nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     36\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(train_input_x)), net(train_input_x)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/sgd.py:185\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     95\u001b[0m SGD\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mImplements stochastic gradient descent (optionally with momentum).\u001b[39m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124m    .. math::\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msgd\u001b[39m(params: List[Tensor],\n\u001b[1;32m    186\u001b[0m         d_p_list: List[Tensor],\n\u001b[1;32m    187\u001b[0m         momentum_buffer_list: List[Optional[Tensor]],\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;66;03m# kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\u001b[39;00m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# setting this as kwarg for now as functional API is compiled by torch/distributed/optim\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         has_sparse_grad: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    191\u001b[0m         foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    193\u001b[0m         weight_decay: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    194\u001b[0m         momentum: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    195\u001b[0m         lr: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    196\u001b[0m         dampening: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    197\u001b[0m         nesterov: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    198\u001b[0m         maximize: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Functional API that performs SGD algorithm computation.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.optim.SGD` for details.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# why must we be explicit about an if statement for torch.jit.is_scripting here?\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# because JIT can't handle Optionals nor fancy conditionals when scripting\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, inp=5000):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(inp, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin1(x).squeeze(-1)\n",
    "\n",
    "net = MyNetwork(train_input_x.size(1)).to(device)\n",
    "opt = optim.SGD(params=net.parameters(), lr=1e-3)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(train_input_x), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "print(epochs)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        cur_train_input_x, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input_x)), net(cur_train_input_x)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input_x)), net(train_input_x)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, n_hidden=30):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin2(F.gelu(self.lin1(x))).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.SGD(params=net.parameters(), lr=1e-3)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(torch.clamp(train_input, 0, 1)), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 5000])\n",
      "torch.float32\n",
      "torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, n_hidden=30):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.gelu(self.lin1(x))\n",
    "        return self.lin2(y).squeeze(-1)\n",
    "    \n",
    "device = 'cuda'\n",
    "dtype = torch.float\n",
    "dataarr_torch = torch.tensor(dataarr, device=device, dtype=dtype).permute(1, 0).contiguous() # (40000, 5000)\n",
    "train_input = dataarr_torch[:20000]\n",
    "print(train_input.shape)\n",
    "print(train_input.dtype)\n",
    "train_labels_torch = torch.tensor(train_labels_arr, device=device, dtype=torch.long) # (20000)\n",
    "print(train_labels_torch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.016871452331543\n",
      "100 0.8875681161880493\n",
      "200 0.7998837232589722\n",
      "300 0.7812641263008118\n",
      "400 0.7742190957069397\n",
      "500 0.7696877121925354\n",
      "600 0.7701017260551453\n",
      "700 0.7680274844169617\n",
      "800 0.7663995027542114\n",
      "900 0.7648466229438782\n",
      "1000 0.7637501955032349\n",
      "1100 0.7622776031494141\n",
      "1200 0.7615422010421753\n",
      "1300 0.760103702545166\n",
      "1400 0.7592649459838867\n",
      "1500 0.7581624388694763\n",
      "1600 0.7573354244232178\n",
      "1700 0.756392776966095\n",
      "1800 0.7555971741676331\n",
      "1900 0.7562779784202576\n",
      "2000 0.7540305852890015\n",
      "2100 0.7533429265022278\n",
      "2200 0.7535622715950012\n",
      "2300 0.751903235912323\n",
      "2400 0.7512956857681274\n",
      "2500 0.7627370953559875\n",
      "2600 0.7500327825546265\n",
      "2700 0.7528928518295288\n",
      "2800 0.7488234639167786\n",
      "2900 0.7482635974884033\n",
      "3000 0.749684751033783\n",
      "3100 0.747818112373352\n",
      "3200 0.7465803027153015\n",
      "3300 0.7460743188858032\n",
      "3400 0.7455999851226807\n",
      "3500 0.7453715801239014\n",
      "3600 0.7445096373558044\n",
      "3700 0.7442658543586731\n",
      "3800 0.7435097694396973\n",
      "3900 0.7430689334869385\n",
      "4000 0.7590693235397339\n",
      "4100 0.7421021461486816\n",
      "4200 0.7416749000549316\n",
      "4300 0.7418526411056519\n",
      "4400 0.7407078742980957\n",
      "4500 0.7402573227882385\n",
      "4600 0.7398225665092468\n",
      "4700 0.7404837012290955\n",
      "4800 0.7418817281723022\n",
      "4900 0.7385188341140747\n",
      "5000 0.73802250623703\n",
      "5100 0.7375980019569397\n",
      "5200 0.7372145652770996\n",
      "5300 0.7368288040161133\n",
      "5400 0.7408936619758606\n",
      "5500 0.746150016784668\n",
      "5600 0.7363739609718323\n",
      "5700 0.7381170988082886\n",
      "5800 0.7444729804992676\n",
      "5900 0.7343193888664246\n",
      "6000 0.7338541150093079\n",
      "6100 0.7334414124488831\n",
      "6200 0.7330583333969116\n",
      "6300 0.7331315279006958\n",
      "6400 0.7322817444801331\n",
      "6500 0.7321143746376038\n",
      "6600 0.7331038117408752\n",
      "6700 0.7323542237281799\n",
      "6800 0.7320511937141418\n",
      "6900 0.7301032543182373\n",
      "7000 0.7374618649482727\n",
      "7100 0.7290819883346558\n",
      "7200 0.7291532754898071\n",
      "7300 0.7292121648788452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30000\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# prob = F.sigmoid(net(train_input))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m20000\u001b[39m), net(train_input)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(result, train_labels_torch) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = MyNetwork().to(device)\n",
    "opt = optim.Adam(params=net.parameters(), lr=1e-3)\n",
    "import math\n",
    "for step in range(30000):\n",
    "    # prob = F.sigmoid(net(train_input))\n",
    "    # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "    result = torch.stack((torch.tensor([0], device=device).repeat(20000), net(train_input)), dim=-1)\n",
    "    loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "    # print(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(step, loss.item())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "0 0.9903290867805481\n",
      "epoch 0 0.8748301863670349\n",
      "epoch 1 0.8618558645248413\n",
      "epoch 2 0.8189964890480042\n",
      "1000 0.8041157722473145\n",
      "epoch 3 0.8064862489700317\n",
      "epoch 4 0.8074341416358948\n",
      "epoch 5 0.7935431599617004\n",
      "2000 0.7592726945877075\n",
      "epoch 6 0.7933539748191833\n",
      "epoch 7 0.7883759140968323\n",
      "epoch 8 0.7852112054824829\n",
      "3000 0.7963464856147766\n",
      "epoch 9 0.7897920608520508\n",
      "epoch 10 0.7860127091407776\n",
      "epoch 11 0.7816511392593384\n",
      "4000 0.8918443918228149\n",
      "epoch 12 0.7812987565994263\n",
      "epoch 13 0.7822210192680359\n",
      "epoch 14 0.7853090763092041\n",
      "5000 0.9241118431091309\n",
      "epoch 15 0.7813931107521057\n",
      "epoch 16 0.7885748147964478\n",
      "epoch 17 0.777962327003479\n",
      "epoch 18 0.7803762555122375\n",
      "6000 0.767274796962738\n",
      "epoch 19 0.77840656042099\n",
      "epoch 20 0.776881992816925\n",
      "epoch 21 0.7754526734352112\n",
      "7000 0.8055574297904968\n",
      "epoch 22 0.7763246893882751\n",
      "epoch 23 0.7747715711593628\n",
      "epoch 24 0.7748647928237915\n",
      "8000 0.6626138091087341\n",
      "epoch 25 0.7836856245994568\n",
      "epoch 26 0.7768938541412354\n",
      "epoch 27 0.7736879587173462\n",
      "9000 0.8921298980712891\n",
      "epoch 28 0.7771432995796204\n",
      "epoch 29 0.7757377028465271\n",
      "epoch 30 0.7823784947395325\n",
      "10000 0.8864059448242188\n",
      "epoch 31 0.7730271220207214\n",
      "epoch 32 0.7839164137840271\n",
      "epoch 33 0.8066873550415039\n",
      "epoch 34 0.7808888554573059\n",
      "11000 0.8530759215354919\n",
      "epoch 35 0.7725895047187805\n",
      "epoch 36 0.7770413160324097\n",
      "epoch 37 0.7719290852546692\n",
      "12000 0.7657548785209656\n",
      "epoch 38 0.7723584175109863\n",
      "epoch 39 0.7723397612571716\n",
      "epoch 40 0.771384596824646\n",
      "13000 0.7122431397438049\n",
      "epoch 41 0.7709336280822754\n",
      "epoch 42 0.7725213170051575\n",
      "epoch 43 0.7908436059951782\n",
      "14000 0.908842921257019\n",
      "epoch 44 0.7763315439224243\n",
      "epoch 45 0.7874739170074463\n",
      "epoch 46 0.7708626985549927\n",
      "15000 0.8215792775154114\n",
      "epoch 47 0.7714874744415283\n",
      "epoch 48 0.7703707218170166\n",
      "epoch 49 0.7693303227424622\n",
      "epoch 50 0.7697716951370239\n",
      "16000 0.8817080855369568\n",
      "epoch 51 0.7724992036819458\n",
      "epoch 52 0.7704924941062927\n",
      "epoch 53 0.7763010263442993\n",
      "17000 0.7700557708740234\n",
      "epoch 54 0.7741696834564209\n",
      "epoch 55 0.7720723152160645\n",
      "epoch 56 0.7693081498146057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# prob = F.sigmoid(net(train_input))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         cur_train_input, cur_train_labels_torch \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m         result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(cur_train_input)), net(cur_train_input)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36mStackDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, n_hidden=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.gelu(self.lin1(x))\n",
    "        return self.lin2(y).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.Adam(params=net.parameters(), lr=1e-5)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(train_input), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "0 0.9906189441680908\n",
      "epoch 0 0.9733400344848633\n",
      "epoch 1 0.9538827538490295\n",
      "epoch 2 0.9386190176010132\n",
      "1000 0.8506855964660645\n",
      "epoch 3 0.9222958087921143\n",
      "epoch 4 0.9097259640693665\n",
      "epoch 5 0.8995742201805115\n",
      "2000 0.8989737629890442\n",
      "epoch 6 0.8890427947044373\n",
      "epoch 7 0.8809101581573486\n",
      "epoch 8 0.8730849027633667\n",
      "3000 0.8120375871658325\n",
      "epoch 9 0.8665003776550293\n",
      "epoch 10 0.8604519963264465\n",
      "epoch 11 0.8547849059104919\n",
      "4000 0.8153812885284424\n",
      "epoch 12 0.8499575257301331\n",
      "epoch 13 0.8453232049942017\n",
      "epoch 14 0.8413929343223572\n",
      "5000 0.8737583756446838\n",
      "epoch 15 0.8376021981239319\n",
      "epoch 16 0.8343681693077087\n",
      "epoch 17 0.8312917947769165\n",
      "epoch 18 0.8295369744300842\n",
      "6000 0.7678480744361877\n",
      "epoch 19 0.8259096741676331\n",
      "epoch 20 0.8232030868530273\n",
      "epoch 21 0.8215454816818237\n",
      "7000 0.9291474223136902\n",
      "epoch 22 0.8189096450805664\n",
      "epoch 23 0.817089319229126\n",
      "epoch 24 0.8151091933250427\n",
      "8000 0.6768872737884521\n",
      "epoch 25 0.813439130783081\n",
      "epoch 26 0.8118957877159119\n",
      "epoch 27 0.8107730746269226\n",
      "9000 0.7910218238830566\n",
      "epoch 28 0.8091257214546204\n",
      "epoch 29 0.8078392148017883\n",
      "epoch 30 0.8065087795257568\n",
      "10000 0.6079807281494141\n",
      "epoch 31 0.8068565726280212\n",
      "epoch 32 0.8042604923248291\n",
      "epoch 33 0.8035336136817932\n",
      "epoch 34 0.8022906184196472\n",
      "11000 0.8119871020317078\n",
      "epoch 35 0.8011583089828491\n",
      "epoch 36 0.8016839027404785\n",
      "epoch 37 0.7994287610054016\n",
      "12000 0.8154126405715942\n",
      "epoch 38 0.7987815737724304\n",
      "epoch 39 0.7979280352592468\n",
      "epoch 40 0.7978867292404175\n",
      "13000 0.7685206532478333\n",
      "epoch 41 0.7968544363975525\n",
      "epoch 42 0.7959564924240112\n",
      "epoch 43 0.7957882285118103\n",
      "14000 0.7462912797927856\n",
      "epoch 44 0.7944300174713135\n",
      "epoch 45 0.7945783138275146\n",
      "epoch 46 0.7970160245895386\n",
      "15000 0.8005892038345337\n",
      "epoch 47 0.79283207654953\n",
      "epoch 48 0.7922343015670776\n",
      "epoch 49 0.7917657494544983\n",
      "epoch 50 0.7913476228713989\n",
      "16000 0.7470858097076416\n",
      "epoch 51 0.7925105690956116\n",
      "epoch 52 0.7909203171730042\n",
      "epoch 53 0.7899664640426636\n",
      "17000 0.7755364179611206\n",
      "epoch 54 0.7894898056983948\n",
      "epoch 55 0.7903710007667542\n",
      "epoch 56 0.7887060046195984\n",
      "18000 0.7663137316703796\n",
      "epoch 57 0.7888413071632385\n",
      "epoch 58 0.7880198359489441\n",
      "epoch 59 0.7876744866371155\n",
      "19000 0.7824024558067322\n",
      "epoch 60 0.7873972654342651\n",
      "epoch 61 0.7871066927909851\n",
      "epoch 62 0.7870200276374817\n",
      "20000 0.8645954728126526\n",
      "epoch 63 0.786689281463623\n",
      "epoch 64 0.7865248918533325\n",
      "epoch 65 0.7865245342254639\n",
      "epoch 66 0.7855157852172852\n",
      "21000 0.9571728706359863\n",
      "epoch 67 0.785261332988739\n",
      "epoch 68 0.7855793237686157\n",
      "epoch 69 0.7856337428092957\n",
      "22000 0.7495459318161011\n",
      "epoch 70 0.785156786441803\n",
      "epoch 71 0.7847014665603638\n",
      "epoch 72 0.7844107747077942\n",
      "23000 0.8139274716377258\n",
      "epoch 73 0.7842526435852051\n",
      "epoch 74 0.7835978269577026\n",
      "epoch 75 0.7894068956375122\n",
      "24000 0.7733199000358582\n",
      "epoch 76 0.7833899855613708\n",
      "epoch 77 0.7829844355583191\n",
      "epoch 78 0.783574640750885\n",
      "25000 0.8448814153671265\n",
      "epoch 79 0.7829281091690063\n",
      "epoch 80 0.7827498912811279\n",
      "epoch 81 0.7822179198265076\n",
      "epoch 82 0.782141387462616\n",
      "26000 0.6956982016563416\n",
      "epoch 83 0.7873098850250244\n",
      "epoch 84 0.782197117805481\n",
      "epoch 85 0.7818706035614014\n",
      "27000 0.7327598333358765\n",
      "epoch 86 0.781383752822876\n",
      "epoch 87 0.7812113165855408\n",
      "epoch 88 0.7814872860908508\n",
      "28000 0.8758179545402527\n",
      "epoch 89 0.7812404036521912\n",
      "epoch 90 0.7810449004173279\n",
      "epoch 91 0.7807450890541077\n",
      "29000 0.7032145857810974\n",
      "epoch 92 0.7811914682388306\n",
      "epoch 93 0.7805819511413574\n",
      "epoch 94 0.7833948135375977\n",
      "30000 0.8117031455039978\n",
      "epoch 95 0.7808749079704285\n",
      "epoch 96 0.7799026370048523\n",
      "epoch 97 0.7803964018821716\n",
      "epoch 98 0.7797141075134277\n",
      "31000 0.909252405166626\n",
      "epoch 99 0.7796769142150879\n",
      "epoch 100 0.7817021608352661\n",
      "epoch 101 0.7793929576873779\n",
      "32000 0.7836823463439941\n",
      "epoch 102 0.7808935642242432\n",
      "epoch 103 0.7794371247291565\n",
      "epoch 104 0.7799343466758728\n",
      "33000 0.6731196045875549\n",
      "epoch 105 0.7793669700622559\n",
      "epoch 106 0.7788940072059631\n",
      "epoch 107 0.7795600295066833\n",
      "34000 0.7623186111450195\n",
      "epoch 108 0.7785056233406067\n",
      "epoch 109 0.7784082293510437\n",
      "epoch 110 0.7837237119674683\n",
      "35000 0.6734473705291748\n",
      "epoch 111 0.7788165211677551\n",
      "epoch 112 0.7813085913658142\n",
      "epoch 113 0.779417872428894\n",
      "epoch 114 0.777959406375885\n",
      "36000 0.7755082249641418\n",
      "epoch 115 0.7779983282089233\n",
      "epoch 116 0.7792716026306152\n",
      "epoch 117 0.779479444026947\n",
      "37000 0.8848742842674255\n",
      "epoch 118 0.7776530981063843\n",
      "epoch 119 0.7775000333786011\n",
      "epoch 120 0.7774892449378967\n",
      "38000 0.6220756769180298\n",
      "epoch 121 0.7779704928398132\n",
      "epoch 122 0.7778580784797668\n",
      "epoch 123 0.7774980664253235\n",
      "39000 0.864517331123352\n",
      "epoch 124 0.7791911959648132\n",
      "epoch 125 0.7773143649101257\n",
      "epoch 126 0.7769116759300232\n",
      "40000 0.8709418773651123\n",
      "epoch 127 0.7768398523330688\n",
      "epoch 128 0.7779994606971741\n",
      "epoch 129 0.7767406105995178\n",
      "41000 0.8304881453514099\n",
      "epoch 130 0.7782490849494934\n",
      "epoch 131 0.7777395844459534\n",
      "epoch 132 0.7768149971961975\n",
      "epoch 133 0.7772693634033203\n",
      "42000 0.7404481768608093\n",
      "epoch 134 0.7763023376464844\n",
      "epoch 135 0.7763732075691223\n",
      "epoch 136 0.7771496176719666\n",
      "43000 0.6919728517532349\n",
      "epoch 137 0.776288628578186\n",
      "epoch 138 0.7760902643203735\n",
      "epoch 139 0.7759770154953003\n",
      "44000 0.7369555234909058\n",
      "epoch 140 0.7764835953712463\n",
      "epoch 141 0.7758289575576782\n",
      "epoch 142 0.7757698893547058\n",
      "45000 0.7525395154953003\n",
      "epoch 143 0.7770123481750488\n",
      "epoch 144 0.7761078476905823\n",
      "epoch 145 0.7756476998329163\n",
      "46000 0.8101559281349182\n",
      "epoch 146 0.7766938209533691\n",
      "epoch 147 0.775580108165741\n",
      "epoch 148 0.7772275805473328\n",
      "epoch 149 0.7755683064460754\n",
      "47000 0.8123915791511536\n",
      "epoch 150 0.7756442427635193\n",
      "epoch 151 0.7775309085845947\n",
      "epoch 152 0.7755666971206665\n",
      "48000 0.8876811265945435\n",
      "epoch 153 0.7757828831672668\n",
      "epoch 154 0.7756811380386353\n",
      "epoch 155 0.7758035659790039\n",
      "49000 0.877609133720398\n",
      "epoch 156 0.7750873565673828\n",
      "epoch 157 0.7752306461334229\n",
      "epoch 158 0.7756722569465637\n",
      "50000 0.7301850318908691\n",
      "epoch 159 0.7753051519393921\n",
      "epoch 160 0.7749377489089966\n",
      "epoch 161 0.7747322916984558\n",
      "51000 0.8523803949356079\n",
      "epoch 162 0.7749261856079102\n",
      "epoch 163 0.775866687297821\n",
      "epoch 164 0.7749019861221313\n",
      "epoch 165 0.7752090096473694\n",
      "52000 0.7610638737678528\n",
      "epoch 166 0.774468719959259\n",
      "epoch 167 0.7744735479354858\n",
      "epoch 168 0.7752794623374939\n",
      "53000 0.8595393300056458\n",
      "epoch 169 0.7765694856643677\n",
      "epoch 170 0.7742531299591064\n",
      "epoch 171 0.7743560671806335\n",
      "54000 0.7443253993988037\n",
      "epoch 172 0.7741665840148926\n",
      "epoch 173 0.7745151519775391\n",
      "epoch 174 0.7741301655769348\n",
      "55000 0.7298400402069092\n",
      "epoch 175 0.7742599248886108\n",
      "epoch 176 0.7784571051597595\n",
      "epoch 177 0.7742443084716797\n",
      "56000 0.864044189453125\n",
      "epoch 178 0.7770270705223083\n",
      "epoch 179 0.773910403251648\n",
      "epoch 180 0.7747581601142883\n",
      "epoch 181 0.77420973777771\n",
      "57000 0.8030416965484619\n",
      "epoch 182 0.773895263671875\n",
      "epoch 183 0.7740153670310974\n",
      "epoch 184 0.773666501045227\n",
      "58000 0.7832168340682983\n",
      "epoch 185 0.7749354243278503\n",
      "epoch 186 0.7743970155715942\n",
      "epoch 187 0.7737064361572266\n",
      "59000 0.8832291960716248\n",
      "epoch 188 0.7734821438789368\n",
      "epoch 189 0.7738366723060608\n",
      "epoch 190 0.7737743854522705\n",
      "60000 0.7715309262275696\n",
      "epoch 191 0.7744811773300171\n",
      "epoch 192 0.773514986038208\n",
      "epoch 193 0.773406982421875\n",
      "61000 0.8880029916763306\n",
      "epoch 194 0.7733280062675476\n",
      "epoch 195 0.7734998464584351\n",
      "epoch 196 0.7732072472572327\n",
      "epoch 197 0.7737330794334412\n",
      "62000 0.8857163786888123\n",
      "epoch 198 0.7743350267410278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin1(x).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.Adam(params=net.parameters(), lr=1e-5)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(train_input), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "0 1.033650517463684\n",
      "epoch 0 0.8794109225273132\n",
      "epoch 1 0.8365387320518494\n",
      "epoch 2 0.8186996579170227\n",
      "1000 0.8073297142982483\n",
      "epoch 3 0.8119237422943115\n",
      "epoch 4 0.8023158311843872\n",
      "epoch 5 0.7970953583717346\n",
      "2000 0.8874736428260803\n",
      "epoch 6 0.7906652092933655\n",
      "epoch 7 0.7920136451721191\n",
      "epoch 8 0.796410083770752\n",
      "3000 0.9649190306663513\n",
      "epoch 9 0.7829987406730652\n",
      "epoch 10 0.7960168719291687\n",
      "epoch 11 0.7821819186210632\n",
      "4000 0.6853991746902466\n",
      "epoch 12 0.7807880640029907\n",
      "epoch 13 0.7816289067268372\n",
      "epoch 14 0.778673529624939\n",
      "5000 0.6736667156219482\n",
      "epoch 15 0.7839246988296509\n",
      "epoch 16 0.7959365844726562\n",
      "epoch 17 0.7756956815719604\n",
      "epoch 18 0.7796845436096191\n",
      "6000 0.830833375453949\n",
      "epoch 19 0.7755473852157593\n",
      "epoch 20 0.774860680103302\n",
      "epoch 21 0.7844651341438293\n",
      "7000 0.7340095639228821\n",
      "epoch 22 0.7737316489219666\n",
      "epoch 23 0.773580014705658\n",
      "epoch 24 0.7740732431411743\n",
      "8000 0.8361451625823975\n",
      "epoch 25 0.7743719816207886\n",
      "epoch 26 0.7927846312522888\n",
      "epoch 27 0.7722731232643127\n",
      "9000 0.9368451833724976\n",
      "epoch 28 0.7734555006027222\n",
      "epoch 29 0.7714782953262329\n",
      "epoch 30 0.7812241315841675\n",
      "10000 0.7749984264373779\n",
      "epoch 31 0.7744967937469482\n",
      "epoch 32 0.7713714838027954\n",
      "epoch 33 0.7707922458648682\n",
      "epoch 34 0.7773739695549011\n",
      "11000 0.7493260502815247\n",
      "epoch 35 0.7716866731643677\n",
      "epoch 36 0.7709202766418457\n",
      "epoch 37 0.7699868679046631\n",
      "12000 0.7160541415214539\n",
      "epoch 38 0.7778339385986328\n",
      "epoch 39 0.7706863284111023\n",
      "epoch 40 0.7692041993141174\n",
      "13000 0.7168422937393188\n",
      "epoch 41 0.7808539271354675\n",
      "epoch 42 0.771176815032959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# prob = F.sigmoid(net(train_input))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         cur_train_input, cur_train_labels_torch \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m         result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(cur_train_input)), net(cur_train_input)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36mStackDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin1(x).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.Adam(params=net.parameters(), lr=1e-4)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(train_input), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "0 1.00606369972229\n",
      "epoch 0 0.9770486354827881\n",
      "epoch 1 0.9636667370796204\n",
      "epoch 2 0.9511532187461853\n",
      "1000 0.922012209892273\n",
      "epoch 3 0.9402894377708435\n",
      "epoch 4 0.9327343702316284\n",
      "epoch 5 0.9217104911804199\n",
      "2000 0.9521302580833435\n",
      "epoch 6 0.9130113124847412\n",
      "epoch 7 0.9056753516197205\n",
      "epoch 8 0.9005245566368103\n",
      "3000 0.8312276601791382\n",
      "epoch 9 0.893158495426178\n",
      "epoch 10 0.8871772289276123\n",
      "epoch 11 0.8832095861434937\n",
      "4000 0.9259429574012756\n",
      "epoch 12 0.8773517608642578\n",
      "epoch 13 0.8729460835456848\n",
      "epoch 14 0.8691934943199158\n",
      "5000 0.8793545961380005\n",
      "epoch 15 0.8663302659988403\n",
      "epoch 16 0.8614073991775513\n",
      "epoch 17 0.8582415580749512\n",
      "epoch 18 0.8550173044204712\n",
      "6000 0.8471183180809021\n",
      "epoch 19 0.8521413207054138\n",
      "epoch 20 0.8502845168113708\n",
      "epoch 21 0.8488448858261108\n",
      "7000 0.9145137667655945\n",
      "epoch 22 0.8445025682449341\n",
      "epoch 23 0.8422002196311951\n",
      "epoch 24 0.8413734436035156\n",
      "8000 0.8456950187683105\n",
      "epoch 25 0.8385442495346069\n",
      "epoch 26 0.8362005949020386\n",
      "epoch 27 0.8343425393104553\n",
      "9000 0.8071707487106323\n",
      "epoch 28 0.8342218995094299\n",
      "epoch 29 0.8309685587882996\n",
      "epoch 30 0.8298436999320984\n",
      "10000 0.8217273354530334\n",
      "epoch 31 0.8281185626983643\n",
      "epoch 32 0.8272792100906372\n",
      "epoch 33 0.8254866003990173\n",
      "epoch 34 0.825065016746521\n",
      "11000 0.680589497089386\n",
      "epoch 35 0.8226282596588135\n",
      "epoch 36 0.8217578530311584\n",
      "epoch 37 0.8203186392784119\n",
      "12000 0.8518446087837219\n",
      "epoch 38 0.8192405700683594\n",
      "epoch 39 0.8183583617210388\n",
      "epoch 40 0.8170932531356812\n",
      "13000 0.7506763935089111\n",
      "epoch 41 0.8161148428916931\n",
      "epoch 42 0.8155922889709473\n",
      "epoch 43 0.814357340335846\n",
      "14000 0.7415325045585632\n",
      "epoch 44 0.8143292665481567\n",
      "epoch 45 0.8125895261764526\n",
      "epoch 46 0.8118085861206055\n",
      "15000 0.8139634132385254\n",
      "epoch 47 0.811029314994812\n",
      "epoch 48 0.8106486201286316\n",
      "epoch 49 0.8097167611122131\n",
      "epoch 50 0.8089013695716858\n",
      "16000 0.8649641275405884\n",
      "epoch 51 0.8082345128059387\n",
      "epoch 52 0.807523787021637\n",
      "epoch 53 0.8070791363716125\n",
      "17000 0.7485540509223938\n",
      "epoch 54 0.8077623248100281\n",
      "epoch 55 0.8057609796524048\n",
      "epoch 56 0.8058848977088928\n",
      "18000 0.8606290221214294\n",
      "epoch 57 0.8068277835845947\n",
      "epoch 58 0.804794192314148\n",
      "epoch 59 0.8039677739143372\n",
      "19000 0.7509877681732178\n",
      "epoch 60 0.8053695559501648\n",
      "epoch 61 0.8040893077850342\n",
      "epoch 62 0.8025747537612915\n",
      "20000 0.7143287658691406\n",
      "epoch 63 0.8016769886016846\n",
      "epoch 64 0.80120849609375\n",
      "epoch 65 0.8006055355072021\n",
      "epoch 66 0.8006641268730164\n",
      "21000 0.8243244290351868\n",
      "epoch 67 0.8009903430938721\n",
      "epoch 68 0.7993279099464417\n",
      "epoch 69 0.7992698550224304\n",
      "22000 0.8671585917472839\n",
      "epoch 70 0.7986286878585815\n",
      "epoch 71 0.79854416847229\n",
      "epoch 72 0.7980442643165588\n",
      "23000 0.8191598653793335\n",
      "epoch 73 0.7976400256156921\n",
      "epoch 74 0.7971013784408569\n",
      "epoch 75 0.7967948317527771\n",
      "24000 0.8538885116577148\n",
      "epoch 76 0.7964470982551575\n",
      "epoch 77 0.7960431575775146\n",
      "epoch 78 0.796811580657959\n",
      "25000 0.6985878348350525\n",
      "epoch 79 0.795779824256897\n",
      "epoch 80 0.7952815890312195\n",
      "epoch 81 0.7955330610275269\n",
      "epoch 82 0.7947264909744263\n",
      "26000 0.7158528566360474\n",
      "epoch 83 0.7951394319534302\n",
      "epoch 84 0.793919026851654\n",
      "epoch 85 0.7936506271362305\n",
      "27000 0.7122383713722229\n",
      "epoch 86 0.7945290803909302\n",
      "epoch 87 0.7933653593063354\n",
      "epoch 88 0.7934182286262512\n",
      "28000 0.8969114422798157\n",
      "epoch 89 0.7926192879676819\n",
      "epoch 90 0.792486310005188\n",
      "epoch 91 0.7922601103782654\n",
      "29000 0.7389476895332336\n",
      "epoch 92 0.792046844959259\n",
      "epoch 93 0.791695237159729\n",
      "epoch 94 0.791589617729187\n",
      "30000 0.8272548317909241\n",
      "epoch 95 0.7924163341522217\n",
      "epoch 96 0.7910526990890503\n",
      "epoch 97 0.7927854061126709\n",
      "epoch 98 0.7915342450141907\n",
      "31000 0.7781113982200623\n",
      "epoch 99 0.7903475761413574\n",
      "epoch 100 0.7900875806808472\n",
      "epoch 101 0.7906726002693176\n",
      "32000 0.7926387786865234\n",
      "epoch 102 0.7897396087646484\n",
      "epoch 103 0.7899597883224487\n",
      "epoch 104 0.7900566458702087\n",
      "33000 0.8321757912635803\n",
      "epoch 105 0.7891258597373962\n",
      "epoch 106 0.7895483374595642\n",
      "epoch 107 0.7887375950813293\n",
      "34000 0.8795703649520874\n",
      "epoch 108 0.7888728976249695\n",
      "epoch 109 0.7884640693664551\n",
      "epoch 110 0.7884407043457031\n",
      "35000 0.8389209508895874\n",
      "epoch 111 0.7882466316223145\n",
      "epoch 112 0.7878417372703552\n",
      "epoch 113 0.7886868119239807\n",
      "epoch 114 0.787866473197937\n",
      "36000 0.7094680666923523\n",
      "epoch 115 0.7874157428741455\n",
      "epoch 116 0.7876627445220947\n",
      "epoch 117 0.7881350517272949\n",
      "37000 0.8212988376617432\n",
      "epoch 118 0.7880144119262695\n",
      "epoch 119 0.7877197265625\n",
      "epoch 120 0.7870532274246216\n",
      "38000 0.81563401222229\n",
      "epoch 121 0.786530077457428\n",
      "epoch 122 0.7867270708084106\n",
      "epoch 123 0.7872925400733948\n",
      "39000 0.8047403693199158\n",
      "epoch 124 0.785998523235321\n",
      "epoch 125 0.7863234281539917\n",
      "epoch 126 0.7860822081565857\n",
      "40000 0.8027365803718567\n",
      "epoch 127 0.7859932780265808\n",
      "epoch 128 0.7854487299919128\n",
      "epoch 129 0.7853166460990906\n",
      "41000 0.8649886250495911\n",
      "epoch 130 0.785597026348114\n",
      "epoch 131 0.7850931286811829\n",
      "epoch 132 0.7858980298042297\n",
      "epoch 133 0.7856471538543701\n",
      "42000 0.852624773979187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# prob = F.sigmoid(net(train_input))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         cur_train_input, cur_train_labels_torch \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m         result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(cur_train_input)), net(cur_train_input)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36mStackDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:255\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: dataset[index] \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin1(x).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.SGD(params=net.parameters(), lr=1e-4)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(train_input), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "0 0.9969463348388672\n",
      "epoch 0 0.9561760425567627\n",
      "epoch 1 0.9374719858169556\n",
      "epoch 2 0.9124813079833984\n",
      "1000 0.9204505085945129\n",
      "epoch 3 0.894507110118866\n",
      "epoch 4 0.8746138215065002\n",
      "epoch 5 0.8537861704826355\n",
      "2000 0.8252146244049072\n",
      "epoch 6 0.8405038118362427\n",
      "epoch 7 0.8273678421974182\n",
      "epoch 8 0.8238879442214966\n",
      "3000 0.7552335858345032\n",
      "epoch 9 0.811022937297821\n",
      "epoch 10 0.8163743019104004\n",
      "epoch 11 0.8022354245185852\n",
      "4000 0.8784327507019043\n",
      "epoch 12 0.8223510980606079\n",
      "epoch 13 0.794126033782959\n",
      "epoch 14 0.8056462407112122\n",
      "5000 0.8217886686325073\n",
      "epoch 15 0.79414302110672\n",
      "epoch 16 0.8037040829658508\n",
      "epoch 17 0.7909581065177917\n",
      "epoch 18 0.7855328917503357\n",
      "6000 0.7806692123413086\n",
      "epoch 19 0.7856742739677429\n",
      "epoch 20 0.7841700315475464\n",
      "epoch 21 0.7824562788009644\n",
      "7000 0.7882317304611206\n",
      "epoch 22 0.7821516394615173\n",
      "epoch 23 0.8118422627449036\n",
      "epoch 24 0.7809096574783325\n",
      "8000 0.8293027281761169\n",
      "epoch 25 0.7811782956123352\n",
      "epoch 26 0.7954747080802917\n",
      "epoch 27 0.7800114154815674\n",
      "9000 0.8193981647491455\n",
      "epoch 28 0.795866072177887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, n_hidden=30):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5000, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin2(F.gelu(self.lin1(x))).squeeze(-1)\n",
    "\n",
    "net = MyNetwork().to(device)\n",
    "opt = optim.SGD(params=net.parameters(), lr=1e-3)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "train_dataloader = DataLoader(StackDataset(TensorDataset(torch.clamp(train_input, 0, 1)), TensorDataset(train_labels_torch)), batch_size=64, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "epochs = 3000000 // len(train_dataloader)\n",
    "\n",
    "import math\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        # prob = F.sigmoid(net(train_input))\n",
    "        # loss = -torch.mean(train_labels_torch * torch.log2(prob) + (1-train_labels_torch) * torch.log2(1-prob))\n",
    "        cur_train_input, cur_train_labels_torch = data[0][0], data[1][0]\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(cur_train_input)), net(cur_train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, cur_train_labels_torch) / math.log(2)\n",
    "        # print(loss)\n",
    "        if global_step % 1000 == 0:\n",
    "            print(global_step, loss.item())\n",
    "        global_step += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        result = torch.stack((torch.tensor([0], device=device).repeat(len(train_input)), net(train_input)), dim=-1)\n",
    "        loss = F.cross_entropy(result, train_labels_torch) / math.log(2)\n",
    "        print(\"epoch\", epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = dataarr_torch[20000:]\n",
    "test_result = net(test_input)\n",
    "df = pd.DataFrame({ 'id': list(range(20000,40000)), 'pred': test_result.cpu().numpy() })"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 4492542,
     "sourceId": 33111,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
